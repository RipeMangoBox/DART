diff --git a/.vscode/launch.json b/.vscode/launch.json
index c15da8b..d8c6cd6 100644
--- a/.vscode/launch.json
+++ b/.vscode/launch.json
@@ -12,7 +12,7 @@
             "cwd": "${workspaceFolder}",
             // 2. 修正所有参数，按空格拆分
             "args": [
-                "--track", "1",
+                "--track", "0",
                 "--exp_name", "mvae_babel_smplx",
                 "--data_args.dataset", "mp_seq_v2",
                 "--data_args.data_dir", "./data/seq_data_zero_male",
@@ -36,21 +36,21 @@
             ]
         },
         {
-            "name": "Train MVAE on FineDance",
+            "name": "Train MPAE on BABEL",
             "type": "debugpy",
             "request": "launch",
-            "program": "mld/train_mvae_fd.py",
+            "program": "mld/train_mpae.py",
             "console": "integratedTerminal",
             // 1. 设置工作目录为项目根目录
             "cwd": "${workspaceFolder}",
             // 2. 修正所有参数，按空格拆分
             "args": [
-                "--track", "1",
+                "--track", "0",
                 "--exp_name", "mvae_babel_smplx",
                 "--data_args.dataset", "mp_seq_v2",
-                "--data_args.data_dir", "./data/seq_data_zero_male_fd",
+                "--data_args.data_dir", "./data/seq_data_zero_male",
                 "--data_args.cfg_path", "./config_files/config_hydra/motion_primitive/mp_h2_f8_r8.yaml",
-                "--data_args.weight_scheme", "uniform",
+                "--data_args.weight_scheme", "text_samp:0.",
                 "--train_args.batch_size", "128",
                 "--train_args.weight_kl", "1e-6",
                 "--train_args.stage1_steps", "100000",
@@ -68,6 +68,73 @@
                 "--model_args.latent_dim", "1", "256"
             ]
         },
+        // {
+        //     "name": "Train MVAE on FineDance",
+        //     "type": "debugpy",
+        //     "request": "launch",
+        //     "program": "mld/train_mvae_fd.py",
+        //     "console": "integratedTerminal",
+        //     // 1. 设置工作目录为项目根目录
+        //     "cwd": "${workspaceFolder}",
+        //     // 2. 修正所有参数，按空格拆分
+        //     "args": [
+        //         "--track", "0",
+        //         "--exp_name", "mvae_babel_smplx",
+        //         "--data_args.dataset", "mp_seq_v2",
+        //         "--data_args.data_dir", "./data/seq_data_zero_male_fd",
+        //         "--data_args.cfg_path", "./config_files/config_hydra/motion_primitive/mp_h2_f8_r8.yaml",
+        //         "--data_args.weight_scheme", "uniform",
+        //         "--train_args.batch_size", "128",
+        //         "--train_args.weight_kl", "1e-6",
+        //         "--train_args.stage1_steps", "100000",
+        //         "--train_args.stage2_steps", "50000",
+        //         "--train_args.stage3_steps", "50000",
+        //         "--train_args.save_interval", "50000",
+        //         "--train_args.weight_smpl_joints_rec", "10.0",
+        //         "--train_args.weight_joints_consistency", "10.0",
+        //         "--train_args.weight_transl_delta", "100",
+        //         "--train_args.weight_joints_delta", "100",
+        //         "--train_args.weight_orient_delta", "100",
+        //         "--model_args.arch", "all_encoder",
+        //         "--train_args.ema_decay", "0.999",
+        //         "--model_args.num_layers", "7",
+        //         "--model_args.latent_dim", "1", "256"
+        //     ]
+        // },
+        {
+            "name": "Train MLD_PAE on BABEL",
+            "type": "debugpy",
+            "request": "launch",
+            "program": "mld/train_mld_pae.py",
+            "console": "integratedTerminal",
+            // 1. 设置工作目录为项目根目录
+            "cwd": "${workspaceFolder}",
+            // 2. 修正所有参数，按空格拆分
+            "args": [
+                // python -m mld.train_mld_pae   
+                "--track", "0",
+                "--exp_name", "mld_pae_babel_smplx",
+                "--data_args.dataset", "mp_seq_v2",
+                "--data_args.data_dir", "./data/seq_data_zero_male",
+                "--data_args.cfg_path", "./config_files/config_hydra/motion_primitive/mp_h2_f8_r4.yaml",
+                "--denoiser_args.mvae_path", "./mvae/mpae_babel_smplx/checkpoint_200000.pt",
+                "--denoiser_args.train_rollout_type", "full",
+                "--denoiser_args.train_rollout_history", "rollout",
+                "--train_args.batch_size", "1024",
+                "--train_args.stage1_steps", "100000",
+                "--train_args.stage2_steps", "100000",
+                "--train_args.stage3_steps", "100000",
+                "--train_args.save_interval", "100000",
+                "--train_args.weight_latent_rec", "1.0",
+                "--train_args.weight_feature_rec", "1.0",
+                "--train_args.weight_smpl_joints_rec", "0",
+                "--train_args.weight_joints_consistency", "0",
+                "--train_args.weight_transl_delta", "1e4",
+                "--train_args.weight_joints_delta", "1e4",
+                "--train_args.weight_orient_delta", "1e4",
+                "--data_args.weight_scheme", "text_samp:0.",
+            ]
+        },
         {
             "name": "Rollout Demo",
             "type": "debugpy",
diff --git a/bash/train_motion_primitive_vae_fd.sh b/bash/train_motion_primitive_vae_fd.sh
deleted file mode 100644
index e45a6d0..0000000
--- a/bash/train_motion_primitive_vae_fd.sh
+++ /dev/null
@@ -1 +0,0 @@
-python -m mld.train_mvae --track 1 --exp_name 'mvae_babel_smplx' --data_args.dataset 'mp_seq_v2' --data_args.data_dir './data/seq_data_zero_male_fd' --data_args.cfg_path './config_files/config_hydra/motion_primitive/mp_h2_f8_r8.yaml' --data_args.weight_scheme 'text_samp:0.' --train_args.batch_size 128  --train_args.weight_kl 1e-6  --train_args.stage1_steps 100000 --train_args.stage2_steps 50000 --train_args.stage3_steps 50000 --train_args.save_interval 50000  --train_args.weight_smpl_joints_rec 10.0 --train_args.weight_joints_consistency 10.0 --train_args.weight_transl_delta 100 --train_args.weight_joints_delta 100 --train_args.weight_orient_delta 100  --model_args.arch 'all_encoder' --train_args.ema_decay 0.999 --model_args.num_layers 7 --model_args.latent_dim 1 256
\ No newline at end of file
diff --git a/mld/train_mvae.py b/mld/train_mvae.py
index ae2b2eb..5fa1a9d 100644
--- a/mld/train_mvae.py
+++ b/mld/train_mvae.py
@@ -366,7 +366,10 @@ class Trainer:
         total_steps = train_args.stage1_steps + train_args.stage2_steps + train_args.stage3_steps
         rest_steps = (total_steps - self.start_step) // self.train_dataset.num_primitive + 1
         rest_steps = rest_steps * self.train_dataset.num_primitive
-        progress_bar = iter(tqdm(range(rest_steps)))
+        # progress_bar = iter(tqdm(range(rest_steps)))
+        # 修改1: 将 tqdm 实例化为对象 pbar，以便后续调用 set_postfix
+        
+        pbar = tqdm(range(rest_steps))
         self.step = self.start_step
         while self.step <= total_steps:
             # Annealing the rate if instructed to do so.
@@ -424,7 +427,11 @@ class Trainer:
                     self.validate()
 
                 self.step += 1
-                next(progress_bar)
+                # next(progress_bar)
+
+                # 修改2: 设置进度条后缀显示 loss，并手动更新进度条
+                pbar.set_postfix({"loss": f"{loss.item():.4f}"})
+                pbar.update(1)
 
     def get_primitive_batch(self, batch, primitive_idx):
         motion = batch[primitive_idx]['motion_tensor_normalized']  # [bs, D, 1, T]
diff --git a/mld/train_mvae_fd.py b/mld/train_mvae_fd.py
deleted file mode 100644
index d2cfaa8..0000000
--- a/mld/train_mvae_fd.py
+++ /dev/null
@@ -1,588 +0,0 @@
-from __future__ import annotations
-
-import os
-os.environ['CUDA_VISIBLE_DEVICES'] = '3'    # must be put here, before importing any other modules
-
-import sys
-sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
-
-import random
-import time
-from dataclasses import dataclass, asdict, make_dataclass
-
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.optim as optim
-from torch.cuda import amp
-import tyro
-import yaml
-from torch.distributions.normal import Normal
-from torch.utils.tensorboard import SummaryWriter
-from pathlib import Path
-from tqdm import tqdm
-import copy
-
-from model.mld_vae import AutoMldVae
-from data_loaders.humanml.data.dataset_fd import PrimitiveSequenceDataset, WeightedPrimitiveSequenceDataset, WeightedPrimitiveSequenceDatasetV2
-from data_loaders.humanml.data.dataset_hml3d import HML3dDataset
-from utilss.smpl_utils import get_smplx_param_from_6d
-from pytorch3d import transforms
-from diffusion.nn import mean_flat, sum_flat
-
-debug = 0
-
-
-
-@dataclass
-class VAEArgs:
-    arch: str = "all_encoder"
-    ff_size: int = 1024
-    num_layers: int = 5
-    num_heads: int = 4
-    dropout: float = 0.1
-    normalize_before: bool = False
-    activation: str = "gelu"
-    position_embedding: str = "learned"
-    latent_dim: tuple[int, int] = (1, 128)
-    h_dim: int = 256
-
-    nfeats: int = 0
-    """feature dimension, will be auto filled"""
-
-@dataclass
-class DataArgs:
-    cfg_path: str = "./config_files/config_hydra/motion_primitive/mp_h2_h8_r1.yaml"
-    """motion primitive config file"""
-
-    data_dir: str = "./data/seq_data"
-    """processed dataset directory"""
-
-    dataset: str = "mp_seq"
-    """dataset name"""
-
-    prob_static: float = 0.0
-    enforce_gender: str = 'male'
-    """enforce all data use the specified gender"""
-
-    enforce_zero_beta: int = 1
-    """enforce all data use zero shape parameters"""
-
-    weight_scheme: str = 'uniform_samp:0.'
-    """weighting schemes determining how motion primitives are sampled during training"""
-
-    text_tolerance: float = 0.0
-    """accept text labels in near future within some frames"""
-
-    history_length: int = 0
-    future_length: int = 0
-    num_primitive: int = 0
-    feature_dim: int = 0
-    """auto filled"""
-
-    body_type: str = 'smplx'
-    """body type, 'smplx' or 'smplh'"""
-
-@dataclass
-class TrainArgs:
-    learning_rate: float = 1e-4
-    anneal_lr: int = 1
-    batch_size: int = 128
-    grad_clip: float = 1.0
-
-    ema_decay: float = 0.999
-    """exponential moving average decay"""
-    use_amp: int = 0
-    """use automatic mixed precision"""
-
-    stage1_steps: int = 100000
-    """training steps for stage 1 without rollout training"""
-    stage2_steps: int = 100000
-    """training steps for stage 2 with linearly increasing percent of rollout training"""
-    stage3_steps: int = 100000
-    """training steps for stage 3 with only rollout training"""
-
-    weight_rec: float = 1.0  # vae only
-    weight_kl: float = 1e-4  # vae only
-    weight_smpl_joints_rec: float = 0.0
-    weight_joints_consistency: float = 0.0
-    weight_transl_delta: float = 0.0
-    weight_orient_delta: float = 0.0
-    weight_joints_delta: float = 0.0
-    weight_latent_rec: float = 1.0  # denoiser only
-    weight_feature_rec: float = 0.0  # denoiser only
-
-    resume_checkpoint: str | None = None
-    log_interval: int = 1000
-    val_interval: int = 10000
-    save_interval: int = 100000
-
-    use_predicted_joints: int = 0
-    """if set to 1, use predicted joints to rollout, otherwise use the regressed joints from smplx body model"""
-
-
-@dataclass
-class Args:
-    train_args: TrainArgs = TrainArgs()
-    model_args: VAEArgs = VAEArgs()
-    data_args: DataArgs = DataArgs()
-
-    exp_name: str = "mvae"
-    seed: int = 0
-    torch_deterministic: bool = True
-    device: str = "cuda"
-    save_dir: str = "./mvae"
-
-    track: int = 1
-    wandb_project_name: str = "mld_vae"
-    wandb_entity: str = "MangoBox"
-
-class Trainer:
-    def __init__(self, args: Args):
-        self.args = args
-        args.save_dir = Path('./mvae') / args.exp_name
-        args.save_dir.mkdir(parents=True, exist_ok=True)
-        train_args = args.train_args
-        model_args = args.model_args
-        data_args = args.data_args
-
-        # TRY NOT TO MODIFY: seeding
-        random.seed(args.seed)
-        np.random.seed(args.seed)
-        torch.manual_seed(args.seed)
-        torch.set_default_dtype(torch.float32)
-        torch.backends.cudnn.deterministic = args.torch_deterministic
-        device = torch.device(args.device if torch.cuda.is_available() else "cpu")
-
-        # load dataset
-        if data_args.dataset == 'mp_seq_v2':
-            dataset_class = WeightedPrimitiveSequenceDatasetV2
-        elif data_args.dataset == 'hml3d':
-            dataset_class = HML3dDataset
-        else:
-            dataset_class = WeightedPrimitiveSequenceDataset
-        train_dataset = dataset_class(dataset_path=data_args.data_dir,
-                                      dataset_name=data_args.dataset,
-                                      cfg_path=data_args.cfg_path, prob_static=data_args.prob_static,
-                                      enforce_gender=data_args.enforce_gender,
-                                      enforce_zero_beta=data_args.enforce_zero_beta,
-                                      body_type=data_args.body_type,
-                                      split='train', device=device,
-                                      weight_scheme=data_args.weight_scheme,
-                                      )
-
-        val_dataset = train_dataset
-        # if 'text' in data_args.weight_scheme or 'samp:1' in data_args.weight_scheme:
-        #     val_dataset = train_dataset
-        # else:
-        #     val_dataset = dataset_class(dataset_path=data_args.data_dir, dataset_name=data_args.dataset,
-        #                                                    cfg_path=data_args.cfg_path, prob_static=data_args.prob_static,
-        #                                                    enforce_gender=data_args.enforce_gender,
-        #                                                    enforce_zero_beta=data_args.enforce_zero_beta,
-        #                                                    split='val', device=device,
-        #                                                    weight_scheme=data_args.weight_scheme,
-        #                                                    )
-
-        # get primitive configs
-        data_args.history_length = train_dataset.history_length
-        data_args.future_length = train_dataset.future_length
-        data_args.num_primitive = train_dataset.num_primitive
-        data_args.feature_dim = 0
-        for k in train_dataset.motion_repr:
-            data_args.feature_dim += train_dataset.motion_repr[k]
-        model_args.nfeats = data_args.feature_dim
-
-        with open(args.save_dir / "args.yaml", "w") as f:
-            yaml.dump(tyro.extras.to_yaml(args), f)
-        with open(args.save_dir / "args_read.yaml", "w") as f:
-            yaml.dump(asdict(args), f)
-        run_name = f"{args.exp_name}__seed{args.seed}__{int(time.time())}"
-        if args.track:
-            import wandb
-            wandb.init(
-                project=args.wandb_project_name,
-                entity=args.wandb_entity,
-                sync_tensorboard=True,
-                config=vars(args),
-                name=run_name,
-                save_code=True,
-            )
-            wandb.run.log_code(root=".",
-                               include_fn=lambda path, root: os.path.relpath(path, root).startswith("mld/") or
-                                                             os.path.relpath(path, root).startswith("model/")
-                               )
-        writer = SummaryWriter(f"runs/{run_name}")
-        writer.add_text(
-            "hyperparameters",
-            "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-        )
-
-        print('model args:', asdict(model_args))
-        model = AutoMldVae(
-            **asdict(model_args),
-        ).to(device)
-        optimizer = optim.AdamW(model.parameters(), lr=train_args.learning_rate)
-        start_step = 1
-        if args.train_args.resume_checkpoint is not None:
-            checkpoint = torch.load(args.train_args.resume_checkpoint)
-            model_state_dict = checkpoint['model_state_dict']
-            if 'latent_mean' not in model_state_dict:
-                model_state_dict['latent_mean'] = torch.tensor(0)
-            if 'latent_std' not in model_state_dict:
-                model_state_dict['latent_std'] = torch.tensor(1)
-            model.load_state_dict(model_state_dict)
-            # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
-            start_step = checkpoint['num_steps'] + 1
-            print(f"Loading checkpoint from {args.train_args.resume_checkpoint} at step {start_step}")
-        self.model_avg = None
-        if args.train_args.ema_decay > 0:
-            self.model_avg = copy.deepcopy(model)
-            self.model_avg.eval()
-
-        self.model = model
-        self.optimizer = optimizer
-        self.writer = writer
-        self.start_step = start_step
-        self.train_dataset = train_dataset
-        self.val_dataset = val_dataset
-        self.device = device
-        self.batch_size = train_args.batch_size
-        self.step = start_step
-
-        self.rec_criterion = torch.nn.HuberLoss(reduction='mean', delta=1.0)
-        self.transf_rotmat = torch.eye(3, device=self.device).unsqueeze(0)
-        self.transf_transl = torch.zeros(3, device=self.device).reshape(1, 1, 3)
-
-    def calc_loss(self, motion, cond, history_motion, future_motion_gt, future_motion_pred, latent, dist):
-        train_args = self.args.train_args
-        model_kwargs = cond
-        future_length = self.train_dataset.future_length
-        history_length = self.train_dataset.history_length
-        num_primitive = self.train_dataset.num_primitive
-
-        terms = {}
-
-        # kl loss
-        mu_ref = torch.zeros_like(dist.loc)
-        scale_ref = torch.ones_like(dist.scale)
-        dist_ref = torch.distributions.Normal(mu_ref, scale_ref)
-        kl_loss = torch.distributions.kl_divergence(dist, dist_ref)
-        kl_loss = kl_loss.mean()
-        terms['kl_loss'] = kl_loss
-
-        # reconstruction loss
-        rec_loss = self.rec_criterion(future_motion_pred, future_motion_gt)
-        terms['rec_loss'] = rec_loss
-
-        """smplx consistency losses"""
-        gt_motion_tensor = future_motion_gt
-        pred_motion_tensor = future_motion_pred
-        genders = model_kwargs['y']['gender']
-        betas = model_kwargs['y']['betas']
-        dataset = self.train_dataset
-        primitive_utility = dataset.primitive_utility
-        def get_smpl_body(motion_tensor, genders, betas):
-            batch_size, num_frames, _ = motion_tensor.shape
-            device = motion_tensor.device
-            smpl_joints = []
-            smpl_vertices = []
-            joints = []
-            for gender_name in ['female', 'male']:
-                # body_model = body_model_male if gender_name == 'male' else body_model_female
-                body_model = primitive_utility.get_smpl_model(gender=gender_name)
-                gender_idx = [idx for idx in range(len(genders)) if genders[idx] == gender_name]
-                sub_batch_size = len(gender_idx)
-                if len(gender_idx) == 0:
-                    continue
-                # gender_betas = betas[gender_idx].unsqueeze(1).repeat(1, num_frames, 1).reshape(
-                #     sub_batch_size * num_frames, -1)
-                gender_betas = betas[gender_idx, history_length:, :].reshape(
-                    sub_batch_size * num_frames, 10)
-                gender_motion_tensor = motion_tensor[gender_idx, :, :]
-                gender_motion_tensor = dataset.denormalize(gender_motion_tensor).reshape(
-                    sub_batch_size * num_frames, -1)
-
-                motion_dict = dataset.tensor_to_dict(gender_motion_tensor)
-                motion_dict.update({'betas': gender_betas})
-                joints.append(motion_dict['joints'].reshape(sub_batch_size, num_frames, 22, 3))
-                smplx_param = get_smplx_param_from_6d(motion_dict)
-                smplxout = body_model(return_verts=False, **smplx_param)
-                smpl_joints.append(smplxout.joints[:, :22, :].reshape(sub_batch_size, num_frames, 22,
-                                                                      3))  # [bs, nframes, 22, 3]
-                # smpl_vertices.append(
-                #     smplxout.vertices.reshape(sub_batch_size, num_frames, -1, 3))  # [bs, nframes, V, 3]
-
-            smpl_joints = torch.cat(smpl_joints, dim=0)
-            # smpl_vertices = torch.cat(smpl_vertices, dim=0)
-            smpl_vertices = None
-            joints = torch.cat(joints, dim=0)
-            return {'smpl_joints': smpl_joints, 'smpl_vertices': smpl_vertices, 'joints': joints}
-
-        with torch.no_grad():
-            gt_result_dict = get_smpl_body(gt_motion_tensor, genders,
-                                           betas)  # note that each batch is reordered according to gender. we assume the input batch is already sorted by gender, so the actual order does not change after this operation
-        pred_result_dict = get_smpl_body(pred_motion_tensor, genders, betas)
-        terms['smpl_joints_rec'] = self.rec_criterion(pred_result_dict['smpl_joints'], gt_result_dict['smpl_joints'])
-        terms['joints_consistency'] = self.rec_criterion(pred_result_dict['joints'], pred_result_dict['smpl_joints'])
-        # terms['smpl_vertices_rec'] = torch.zeros_like(terms['smpl_joints_rec'])
-
-        """temporal delta loss"""
-        pred_motion_tensor = torch.cat([history_motion[:, -1:, :], future_motion_pred], dim=1)  # [B, F+1, D]
-        pred_motion_tensor = dataset.denormalize(pred_motion_tensor)
-        pred_feature_dict = dataset.tensor_to_dict(pred_motion_tensor)
-        pred_joints_delta = pred_feature_dict['joints_delta'][:, :-1, :]
-        pred_transl_delta = pred_feature_dict['transl_delta'][:, :-1, :]
-        pred_orient_delta = pred_feature_dict['global_orient_delta_6d'][:, :-1, :]
-        calc_joints_delta = pred_feature_dict['joints'][:, 1:, :] - pred_feature_dict['joints'][:, :-1, :]
-        calc_transl_delta = pred_feature_dict['transl'][:, 1:, :] - pred_feature_dict['transl'][:, :-1, :]
-        pred_orient = transforms.rotation_6d_to_matrix(pred_feature_dict['poses_6d'][:, :, :6])  # [B, T, 3, 3]
-        calc_orient_delta_matrix = torch.matmul(pred_orient[:, 1:],
-                                                pred_orient[:, :-1].permute(0, 1, 3, 2))
-        calc_orient_delta_6d = transforms.matrix_to_rotation_6d(calc_orient_delta_matrix)
-        terms["joints_delta"] = self.rec_criterion(calc_joints_delta, pred_joints_delta)
-        terms["transl_delta"] = self.rec_criterion(calc_transl_delta, pred_transl_delta)
-        terms["orient_delta"] = self.rec_criterion(calc_orient_delta_6d, pred_orient_delta)
-
-        loss = train_args.weight_kl * kl_loss + train_args.weight_rec * rec_loss + \
-               train_args.weight_smpl_joints_rec * terms['smpl_joints_rec'] + \
-               train_args.weight_joints_consistency * terms['joints_consistency'] + \
-               train_args.weight_joints_delta * terms["joints_delta"] + \
-               train_args.weight_transl_delta * terms["transl_delta"] + \
-               train_args.weight_orient_delta * terms["orient_delta"]
-        terms['loss'] = loss
-        return terms
-
-    def train(self):
-        model = self.model
-        optimizer = self.optimizer
-        args = self.args
-        train_args = self.args.train_args
-        writer = self.writer
-        future_length = self.train_dataset.future_length
-        history_length = self.train_dataset.history_length
-        num_primitive = self.train_dataset.num_primitive
-
-        model.train()
-        total_steps = train_args.stage1_steps + train_args.stage2_steps + train_args.stage3_steps
-        rest_steps = (total_steps - self.start_step) // self.train_dataset.num_primitive + 1
-        rest_steps = rest_steps * self.train_dataset.num_primitive
-        progress_bar = iter(tqdm(range(rest_steps)))
-        self.step = self.start_step
-        while self.step <= total_steps:
-            # Annealing the rate if instructed to do so.
-            if train_args.anneal_lr:
-                frac = 1.0 - (self.step - 1.0) / total_steps
-                lrnow = frac * train_args.learning_rate
-                optimizer.param_groups[0]["lr"] = lrnow
-
-            with amp.autocast(enabled=bool(train_args.use_amp), dtype=torch.float16):
-                batch = self.train_dataset.get_batch(self.batch_size)
-            last_primitive = None
-            for primitive_idx in range(num_primitive):
-                with amp.autocast(enabled=bool(train_args.use_amp), dtype=torch.float16):
-                    motion, cond = self.get_primitive_batch(batch, primitive_idx)
-                    motion_tensor = motion.squeeze(2).permute(0, 2, 1)  # [B, T, D]
-                    future_motion_gt = motion_tensor[:, -future_length:, :]
-                    history_motion = motion_tensor[:, :history_length, :]
-                    if last_primitive is not None:
-                        rollout_history = self.get_rollout_history(last_primitive, cond)
-                        history_motion = rollout_history    # [B, H, D]
-
-                    latent, dist = model.encode(future_motion=future_motion_gt, history_motion=history_motion)
-                    future_motion_pred = model.decode(latent, history_motion, nfuture=future_length)  # [B, F, D]
-
-                    loss_dict = self.calc_loss(motion, cond, history_motion, future_motion_gt, future_motion_pred, latent, dist)
-                    loss = loss_dict['loss']
-
-                optimizer.zero_grad()
-                loss.backward()
-                nn.utils.clip_grad_norm_(model.parameters(), train_args.grad_clip)
-                optimizer.step()
-
-                # update the average model using exponential moving average
-                if train_args.ema_decay > 0:
-                    for param, avg_param in zip(self.model.parameters(), self.model_avg.parameters()):
-                        avg_param.data.mul_(train_args.ema_decay).add_(
-                            param.data, alpha=1 - train_args.ema_decay)
-
-                last_primitive = None
-                if self.step > train_args.stage1_steps:
-                    rollout_prob = min(1.0, (self.step - train_args.stage1_steps) / max(
-                        float(train_args.stage2_steps), 1e-6))
-                    if torch.rand(1).item() < rollout_prob:
-                        last_primitive = future_motion_pred.detach()
-
-                if self.step % train_args.log_interval == 0:
-                    for key in loss_dict:
-                        writer.add_scalar(f"loss/{key}", loss_dict[key].item(), self.step)
-                    writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], self.step)
-
-                if self.step % train_args.save_interval == 0 or self.step == total_steps:
-                    self.save()
-
-                if self.step % train_args.val_interval == 0 or self.step == total_steps:
-                    self.validate()
-
-                self.step += 1
-                next(progress_bar)
-
-    def get_primitive_batch(self, batch, primitive_idx):
-        motion = batch[primitive_idx]['motion_tensor_normalized']  # [bs, D, 1, T]
-        cond = {'y': {'text': batch[primitive_idx]['texts'],
-                      'text_embedding': batch[primitive_idx]['text_embedding'],  # [bs, 512]
-                      'gender': batch[primitive_idx]['gender'],
-                      'betas': batch[primitive_idx]['betas'],  # [bs, T, 10]
-                      'history_motion': batch[primitive_idx]['history_motion'],  # [bs, D, 1, T]
-                      'history_mask': batch[primitive_idx]['history_mask'],
-                      'history_length': batch[primitive_idx]['history_length'],
-                      'future_length': batch[primitive_idx]['future_length']
-                      }
-                }
-        return motion, cond
-
-    def get_rollout_history(self, last_primitive, cond,
-                            return_transform=False,
-                            transf_rotmat=None, transf_transl=None
-                            ):
-        """update history motion seed, update global transform"""
-        motion_tensor = last_primitive[:, -self.train_dataset.history_length:, :]  # [B, T, D]
-        new_history_frames = self.train_dataset.denormalize(motion_tensor)
-        primitive_utility = self.train_dataset.primitive_utility
-        rollout_history = []
-        genders = cond['y']['gender']
-        new_transf_rotmat, new_transf_transl = [], []
-        for gender_name in ['female', 'male']:
-            gender_idx = [idx for idx in range(len(genders)) if genders[idx] == gender_name]
-            if len(gender_idx) == 0:
-                continue
-            history_feature_dict = primitive_utility.tensor_to_dict(new_history_frames[gender_idx])
-            history_feature_dict.update(
-                {
-                    'transf_rotmat': self.transf_rotmat.repeat(len(gender_idx), 1, 1) if transf_rotmat is None else transf_rotmat[gender_idx],
-                    'transf_transl': self.transf_transl.repeat(len(gender_idx), 1, 1) if transf_transl is None else transf_transl[gender_idx],
-                    'gender': gender_name,
-                    'betas': cond['y']['betas'][gender_idx, -self.train_dataset.history_length:, :],
-                }
-            )
-            pelvis_delta = primitive_utility.calc_calibrate_offset({
-                'betas': history_feature_dict['betas'][:, 0, :],  # [B, 10]
-                'gender': gender_name,
-            })
-            history_feature_dict['pelvis_delta'] = pelvis_delta
-            use_predicted_joints = getattr(self.args.train_args, 'use_predicted_joints', False)
-            canonicalized_history_primitive_dict, blended_feature_dict = primitive_utility.get_blended_feature(
-                history_feature_dict, use_predicted_joints=use_predicted_joints)
-            new_transf_rotmat.append(canonicalized_history_primitive_dict['transf_rotmat'])
-            new_transf_transl.append(canonicalized_history_primitive_dict['transf_transl'])
-            history_motion_tensor = primitive_utility.dict_to_tensor(blended_feature_dict)
-            rollout_history.append(history_motion_tensor)
-
-        rollout_history = torch.cat(rollout_history, dim=0)
-        rollout_history = self.train_dataset.normalize(rollout_history)  # [B, T, D]
-        # rollout_history = rollout_history.permute(0, 2, 1).unsqueeze(2)  # [B, D, 1, T_history]
-
-        if return_transform:
-            return rollout_history, torch.cat(new_transf_rotmat, dim=0), torch.cat(new_transf_transl, dim=0)
-        else:
-            return rollout_history
-
-    def get_latent_scale(self, model):
-        """
-        get the scale of the latent space
-            model: model or model_avg
-        """
-        original_mode = model.training
-        model.eval()
-
-        train_args = self.args.train_args
-        future_length = self.train_dataset.future_length
-        history_length = self.train_dataset.history_length
-        num_primitive = self.train_dataset.num_primitive
-
-        with torch.no_grad():
-            batch = self.train_dataset.get_batch(self.batch_size)
-            primitive_idx = 0
-            motion, cond = self.get_primitive_batch(batch, primitive_idx)
-            motion_tensor = motion.squeeze(2).permute(0, 2, 1)  # [B, T, D]
-            future_motion_gt = motion_tensor[:, -future_length:, :]
-            history_motion = motion_tensor[:, :history_length, :]
-
-            latent, dist = model.encode(future_motion=future_motion_gt, history_motion=history_motion)  # [1, B, D]
-            all_mean = latent.mean()
-            all_std = (latent - all_mean).pow(2).mean().sqrt()
-            model.register_buffer("latent_mean", all_mean)
-            model.register_buffer("latent_std", all_std)
-            print(f"latent mean: {all_mean}, latent std: {all_std}")
-
-        model.train(original_mode)
-
-    def save(self):
-        model = self.model if self.model_avg is None else self.model_avg
-        print('save avg model:', self.model_avg is not None)
-        self.get_latent_scale(model)
-        checkpoint_path = self.args.save_dir / f"checkpoint_{self.step}.pt"
-        torch.save({
-            'num_steps': self.step,
-            'model_state_dict': model.state_dict(),
-            # 'optimizer_state_dict': self.optimizer.state_dict(),
-        }, checkpoint_path)
-        print(f"Saved checkpoint at {checkpoint_path}")
-
-    def validate(self):
-        original_mode = self.model.training
-        self.model.eval()
-
-        model = self.model
-        optimizer = self.optimizer
-        args = self.args
-        train_args = self.args.train_args
-        writer = self.writer
-        future_length = self.train_dataset.future_length
-        history_length = self.train_dataset.history_length
-        num_primitive = self.train_dataset.num_primitive
-
-        with torch.no_grad():
-            losses_dict = {}
-            for _ in tqdm(range(max(128, len(self.val_dataset) // self.batch_size))):
-                batch = self.val_dataset.get_batch(self.batch_size)
-                last_primitive = None
-                for primitive_idx in range(num_primitive):
-                    motion, cond = self.get_primitive_batch(batch, primitive_idx)
-                    motion_tensor = motion.squeeze(2).permute(0, 2, 1)  # [B, T, D]
-                    future_motion_gt = motion_tensor[:, -future_length:, :]
-                    history_motion = motion_tensor[:, :history_length, :]
-                    if last_primitive is not None:
-                        rollout_history = self.get_rollout_history(last_primitive, cond)
-                        history_motion = rollout_history  # [B, H, D]
-
-                    latent, dist = model.encode(future_motion=future_motion_gt, history_motion=history_motion)
-                    future_motion_pred = model.decode(latent, history_motion, nfuture=future_length)
-
-                    loss_dict = self.calc_loss(motion, cond, history_motion, future_motion_gt, future_motion_pred,
-                                               latent, dist)
-                    for k, v in loss_dict.items():
-                        if k not in losses_dict:
-                            losses_dict[k] = []
-                        losses_dict[k].append(v.detach())
-
-                    if self.step > train_args.stage1_steps:
-                        last_primitive = future_motion_pred.detach()
-                    else:
-                        last_primitive = None
-
-        for k, v in losses_dict.items():
-            losses_dict[k] = torch.stack(v).mean().item()
-            self.writer.add_scalar(f"val_loss/{k}", losses_dict[k], self.step)
-        self.model.train(original_mode)
-
-    def close(self):
-        self.writer.close()
-
-
-
-if __name__ == "__main__":
-    args = tyro.cli(Args)
-    trainer = Trainer(args)
-    trainer.train()
-    trainer.close()
-
diff --git a/model/mld_vae.py b/model/mld_vae.py
index 06e0ab4..e2b500b 100644
--- a/model/mld_vae.py
+++ b/model/mld_vae.py
@@ -142,7 +142,7 @@ class AutoMldVae(nn.Module):
         # resampling
         std = logvar.exp().pow(0.5)
         dist = torch.distributions.Normal(mu, std)
-        latent = dist.rsample()
+        latent = dist.rsample() # (1, bs, latent_dim)
         if scale_latent:  # only used during denoiser training
             latent = latent / self.latent_std
         return latent, dist
@@ -179,3 +179,233 @@ class AutoMldVae(nn.Module):
         # Pytorch Transformer: [Sequence, Batch size, ...]
         feats = output.permute(1, 0, 2)
         return feats
+
+class AutoMldPae(nn.Module):
+    def __init__(self,
+                 nfeats: int,
+                 latent_dim: tuple = [1, 256], # PAE模式下，latent_dim[-1] 应与 PAE 的 latent_dim 对应
+                 h_dim: int = 512,
+                 ff_size: int = 1024,
+                 num_layers: int = 9,
+                 num_heads: int = 4,
+                 dropout: float = 0.1,
+                 time_range: int = 10, # 对应 PAE 的 window size
+                 window: float = 1/3,   # 时间窗口长度（秒）
+                 pae_combine_mode: str = "linear",
+                 arch: str = "all_encoder",
+                 position_embedding: str = "learned",
+                 normalize_before: bool = False,
+                 activation: str = "gelu",
+                 **kwargs) -> None:
+        super().__init__()
+
+        self.h_dim = h_dim
+        self.time_range = time_range
+        self.latent_dim = latent_dim[-1] # PAE 的分量数
+        self.latent_size = latent_dim[0]
+        self.arch = arch
+
+        self.query_pos_encoder = build_position_encoding(self.h_dim, position_embedding=position_embedding)
+        self.query_pos_decoder = build_position_encoding(self.h_dim, position_embedding=position_embedding)
+        
+        # --- 1. 引入 PAE 专有的物理参数 ---
+        self.tpi = nn.Parameter(torch.tensor([2.0 * np.pi]), requires_grad=False)
+        self.args = nn.Parameter(torch.linspace(-window/2, window/2, time_range), requires_grad=False)
+        self.freqs = nn.Parameter(torch.fft.rfftfreq(time_range)[1:] * time_range / window, requires_grad=False)
+
+        # --- 2. 编码/解码结构 ---
+        if pae_combine_mode == "conv":
+            self.skel_embedding = ConvFeatureExtractor(nfeats, h_dim, time_range)
+        else:
+            self.skel_embedding = nn.Linear(nfeats, h_dim)
+            
+        self.decoder_latent_proj = nn.Linear(self.latent_dim, self.h_dim)
+
+        encoder_layer = TransformerEncoderLayer(
+            self.h_dim,
+            num_heads,
+            ff_size,
+            dropout,
+            activation,
+            normalize_before,
+        )
+        encoder_norm = nn.LayerNorm(self.h_dim)
+        self.encoder = SkipTransformerEncoder(encoder_layer, num_layers, encoder_norm)
+        self.encoder_latent_proj = nn.Linear(self.h_dim, self.latent_dim)
+
+        if self.arch == "all_encoder":
+            decoder_norm = nn.LayerNorm(self.h_dim)
+            self.decoder = SkipTransformerEncoder(encoder_layer, num_layers, decoder_norm)
+        elif self.arch == "encoder_decoder":
+            decoder_layer = TransformerDecoderLayer(
+                self.h_dim,
+                num_heads,
+                ff_size,
+                dropout,
+                activation,
+                normalize_before,
+            )
+            decoder_norm = nn.LayerNorm(self.h_dim)
+            self.decoder = SkipTransformerDecoder(decoder_layer, num_layers, decoder_norm)
+        else:
+            raise ValueError("Not support architecture!")
+        
+        # Transformer 用于处理时间序列
+        # 注意：PAE 的逻辑通常基于 Encoder 提取的特征 y 来计算相位
+        encoder_layer = nn.TransformerEncoderLayer(d_model=h_dim, nhead=4, dim_feedforward=1024, batch_first=False)
+        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)
+        
+        # 将 Transformer 输出映射到 PAE 的 embedding 通道
+        self.to_phase_latent = nn.Linear(h_dim, self.latent_dim)
+
+        # PAE 关键：相位投影 (对应 PAE 的 self.fc)
+        self.phase_fc = nn.ModuleList([nn.Linear(time_range, 2) for _ in range(self.latent_dim)])
+
+        # --- 3. 后处理结构 ---
+        self.final_layer = nn.Linear(self.latent_dim, nfeats)
+        
+        self.register_buffer('latent_mean', torch.tensor(0))
+        self.register_buffer('latent_std', torch.tensor(1))
+
+    def FFT(self, y, dim=2):
+        """ 这里的 y 形状为 [bs, latent_dim, time_range] """
+        # 1. 记录原始数据类型
+        orig_dtype = y.dtype
+        
+        # 2. 将输入转换为 float32 进行计算
+        # 这样可以绕过 cuFFT 对半精度必须是 2 的幂次方的限制
+        y_float32 = y.to(torch.float32)
+        
+        # 3. 执行 FFT
+        rfft = torch.fft.rfft(y_float32, dim=dim)
+        magnitudes = rfft.abs()
+        spectrum = magnitudes[:, :, 1:] 
+        power = spectrum**2
+
+        # 频率 (Weighted average frequency)
+        freq = torch.sum(self.freqs * power, dim=dim) / (torch.sum(power, dim=dim) + 1e-8)
+        # 振幅
+        amp = 2 * torch.sqrt(torch.sum(power, dim=dim)) / self.time_range
+        # 偏移 (DC Component)
+        offset = rfft.real[:, :, 0] / self.time_range
+        return freq, amp, offset
+
+    def encode(self, future_motion, history_motion, scale_latent: bool = False):
+        bs = future_motion.shape[0]
+        # 合并历史与未来 [bs, seq, feats]
+        x = torch.cat((history_motion, future_motion), dim=1) 
+        
+        # 1. 语义浓缩 (Linear) -> [bs, seq, h_dim]
+        x = self.skel_embedding(x)
+        
+        # 2. Transformer 时间建模
+        y = self.encoder(x) # [bs, seq, h_dim]
+        
+        # 3. 映射到 PAE 隐空间 [bs, seq, h_dim] -> [bs, latent_dim, seq]
+        y = self.to_phase_latent(y).permute(0, 2, 1)
+        
+        latent = y #Save latent for returning
+        
+        # 4. 计算 PAE 参数 (FFT)
+        f, a, b = self.FFT(y, dim=2) # [bs, emb_ch]
+        
+        # 5. 计算相位 (Atan2)
+        p = torch.empty((bs, self.latent_dim), device=y.device)
+        for i in range(self.latent_dim):
+            v = self.phase_fc[i](y[:, i, :]) # 输入整个时间窗口
+            p[:, i] = torch.atan2(v[:, 1], v[:, 0]) / self.tpi
+
+        #Parameters    
+        # p = p.unsqueeze(2)  # (bs, latent_dim, 1)
+        # f = f.unsqueeze(2)  # (bs, latent_dim, 1)
+        # a = a.unsqueeze(2)  # (bs, latent_dim, 1)
+        # b = b.unsqueeze(2)  # (bs, latent_dim, 1)
+        # # params = [p, f, a, b]
+        params = torch.cat([p, f, a, b], dim=1).unsqueeze(0) # (1, bs,  4 *latent_dim)
+        # params = torch.stack([p, f, a, b], dim=2)
+        
+        # signal = a * torch.sin(self.tpi * (f * self.args + p)) + b
+
+        return params, latent
+
+    def decode(self, params: Tensor, history_motion, nfuture,
+               scale_latent: bool = False,
+               ):
+        bs = history_motion.shape[0]
+            
+        """ 接收 PAE 参数进行正弦重建 """
+        p, f, a, b = params.permute(1, 2, 0).split(self.latent_dim, dim=1) # 4 * (bs, latent_dim, 1)
+
+        # 1. 信号重建 (PAE 核心公式)
+        # y: [bs, latent_dim, time_range]
+        y = a * torch.sin(self.tpi * (f * self.args + p)) + b
+        
+        # 2. 映射回特征空间 [bs, emb, seq] -> [seq, bs, emb]
+        y = y.permute(2, 0, 1)
+        
+        y = self.decoder_latent_proj(y)  # [latent_size, bs, latent_dim] => [latent_size, bs, h_dim]
+        queries = torch.zeros(nfuture, bs, self.h_dim, device=y.device)
+        history_embedding = self.skel_embedding(history_motion).permute(1, 0, 2)  # [nhistory, bs, h_dim]
+
+        # Pass through the transformer decoder
+        # with the latent vector for memory
+        if self.arch == "all_encoder":
+            xseq = torch.cat((y, history_embedding, queries), dim=0)
+            xseq = self.query_pos_decoder(xseq)
+            output = self.decoder(
+                xseq)[-nfuture:]
+
+        elif self.arch == "encoder_decoder":
+            xseq = torch.cat((history_embedding, queries), dim=0)
+            xseq = self.query_pos_decoder(xseq)
+            output = self.decoder(
+                tgt=xseq,
+                memory=y,
+            )
+            # print('output:', output.shape)
+            output = output[-nfuture:]
+
+        output = self.final_layer(output)
+        # Pytorch Transformer: [Sequence, Batch size, ...]
+        feats = output.permute(1, 0, 2)
+        return feats
+    
+    
+    # def decode(self, params, nfuture):
+    #     """ 接收 PAE 参数进行正弦重建 """
+    #     p, f, a, b = params.permute(1, 2, 0).split(self.latent_dim, dim=1) # 4 * (bs, latent_dim, 1)
+
+    #     # 1. 信号重建 (PAE 核心公式)
+    #     # y: [bs, latent_dim, time_range]
+    #     y = a * torch.sin(self.tpi * (f * self.args + p)) + b
+        
+    #     # 2. 映射回特征空间 [bs, emb, seq] -> [seq, bs, emb]
+    #     y = y.permute(2, 0, 1)
+        
+    #     # 3. 最终投影到运动维度 [seq, bs, nfeats]
+    #     output = self.final_layer(y)
+        
+    #     # 返回最后 nfuture 帧 [bs, nfuture, nfeats]
+    #     return output[-nfuture:].permute(1, 0, 2)
+
+class ConvFeatureExtractor(nn.Module):
+    def __init__(self, input_channels, h_dim, time_range):
+        super().__init__()
+        # 借鉴 Code 2 的结构
+        intermediate = input_channels // 3
+        padding = (time_range - 1) // 2
+        
+        self.net = nn.Sequential(
+            nn.Conv1d(input_channels, intermediate, time_range, stride=1, padding=padding),
+            nn.LayerNorm([intermediate, time_range]), # 注意 LN 在 1D 上的维度
+            nn.ELU(),
+            nn.Conv1d(intermediate, h_dim, time_range, stride=1, padding=padding),
+            nn.ELU()
+        )
+
+    def forward(self, x):
+        # x: [bs, seq_len, nfeats] -> [bs, nfeats, seq_len]
+        x = x.permute(0, 2, 1)
+        x = self.net(x)
+        # -> [bs, h_dim, seq_len] -> [seq_len, bs, h_dim]
+        return x.permute(2, 0, 1)
\ No newline at end of file
diff --git a/mvae/mvae_babel_smplx/args.yaml b/mvae/mvae_babel_smplx/args.yaml
index 3db2f1c..e905335 100644
--- a/mvae/mvae_babel_smplx/args.yaml
+++ b/mvae/mvae_babel_smplx/args.yaml
@@ -1,19 +1,18 @@
 "# tyro YAML.\n!dataclass:Args\ndata_args: !dataclass:DataArgs\n  body_type: smplx\n\
   \  cfg_path: ./config_files/config_hydra/motion_primitive/mp_h2_f8_r8.yaml\n  data_dir:\
-  \ ./data/seq_data_zero_male_fd\n  dataset: mp_seq_v2\n  enforce_gender: male\n \
-  \ enforce_zero_beta: 1\n  feature_dim: 276\n  future_length: 8\n  history_length:\
-  \ 2\n  num_primitive: 8\n  prob_static: 0.0\n  text_tolerance: 0.0\n  weight_scheme:\
-  \ uniform\ndevice: cuda\nexp_name: mvae_babel_smplx\nmodel_args: !dataclass:VAEArgs\n\
-  \  activation: gelu\n  arch: all_encoder\n  dropout: 0.1\n  ff_size: 1024\n  h_dim:\
-  \ 256\n  latent_dim: !!python/tuple\n  - 1\n  - 256\n  nfeats: 276\n  normalize_before:\
-  \ false\n  num_heads: 4\n  num_layers: 7\n  position_embedding: learned\nsave_dir:\
-  \ !!python/object/apply:pathlib.PosixPath\n- mvae\n- mvae_babel_smplx\nseed: 0\n\
-  torch_deterministic: true\ntrack: 1\ntrain_args: !dataclass:TrainArgs\n  anneal_lr:\
-  \ 1\n  batch_size: 128\n  ema_decay: 0.999\n  grad_clip: 1.0\n  learning_rate: 0.0001\n\
-  \  log_interval: 1000\n  resume_checkpoint: null\n  save_interval: 50000\n  stage1_steps:\
-  \ 100000\n  stage2_steps: 50000\n  stage3_steps: 50000\n  use_amp: 0\n  use_predicted_joints:\
-  \ 0\n  val_interval: 10000\n  weight_feature_rec: 0.0\n  weight_joints_consistency:\
-  \ 10.0\n  weight_joints_delta: 100.0\n  weight_kl: 1.0e-06\n  weight_latent_rec:\
-  \ 1.0\n  weight_orient_delta: 100.0\n  weight_rec: 1.0\n  weight_smpl_joints_rec:\
-  \ 10.0\n  weight_transl_delta: 100.0\nwandb_entity: MangoBox\nwandb_project_name:\
-  \ mld_vae\n"
+  \ ./data/seq_data_zero_male\n  dataset: mp_seq_v2\n  enforce_gender: male\n  enforce_zero_beta:\
+  \ 1\n  feature_dim: 276\n  future_length: 8\n  history_length: 2\n  num_primitive:\
+  \ 8\n  prob_static: 0.0\n  text_tolerance: 0.0\n  weight_scheme: text_samp:0.\n\
+  device: cuda\nexp_name: mvae_babel_smplx\nmodel_args: !dataclass:VAEArgs\n  activation:\
+  \ gelu\n  arch: all_encoder\n  dropout: 0.1\n  ff_size: 1024\n  h_dim: 256\n  latent_dim:\
+  \ !!python/tuple\n  - 1\n  - 256\n  nfeats: 276\n  normalize_before: false\n  num_heads:\
+  \ 4\n  num_layers: 7\n  position_embedding: learned\nsave_dir: !!python/object/apply:pathlib.PosixPath\n\
+  - mvae\n- mvae_babel_smplx\nseed: 0\ntorch_deterministic: true\ntrack: 0\ntrain_args:\
+  \ !dataclass:TrainArgs\n  anneal_lr: 1\n  batch_size: 128\n  ema_decay: 0.999\n\
+  \  grad_clip: 1.0\n  learning_rate: 0.0001\n  log_interval: 1000\n  resume_checkpoint:\
+  \ null\n  save_interval: 50000\n  stage1_steps: 100000\n  stage2_steps: 50000\n\
+  \  stage3_steps: 50000\n  use_amp: 0\n  use_predicted_joints: 0\n  val_interval:\
+  \ 10000\n  weight_feature_rec: 0.0\n  weight_joints_consistency: 10.0\n  weight_joints_delta:\
+  \ 100.0\n  weight_kl: 1.0e-06\n  weight_latent_rec: 1.0\n  weight_orient_delta:\
+  \ 100.0\n  weight_rec: 1.0\n  weight_smpl_joints_rec: 10.0\n  weight_transl_delta:\
+  \ 100.0\nwandb_entity: MangoBox\nwandb_project_name: mld_vae\n"
diff --git a/mvae/mvae_babel_smplx/args_read.yaml b/mvae/mvae_babel_smplx/args_read.yaml
index a247ed4..0ea4cb6 100644
--- a/mvae/mvae_babel_smplx/args_read.yaml
+++ b/mvae/mvae_babel_smplx/args_read.yaml
@@ -1,7 +1,7 @@
 data_args:
   body_type: smplx
   cfg_path: ./config_files/config_hydra/motion_primitive/mp_h2_f8_r8.yaml
-  data_dir: ./data/seq_data_zero_male_fd
+  data_dir: ./data/seq_data_zero_male
   dataset: mp_seq_v2
   enforce_gender: male
   enforce_zero_beta: 1
@@ -11,7 +11,7 @@ data_args:
   num_primitive: 8
   prob_static: 0.0
   text_tolerance: 0.0
-  weight_scheme: uniform
+  weight_scheme: text_samp:0.
 device: cuda
 exp_name: mvae_babel_smplx
 model_args:
@@ -33,7 +33,7 @@ save_dir: !!python/object/apply:pathlib.PosixPath
 - mvae_babel_smplx
 seed: 0
 torch_deterministic: true
-track: 1
+track: 0
 train_args:
   anneal_lr: 1
   batch_size: 128
